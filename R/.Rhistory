chinese_cols <- grep("^Industry_en", colnames(df), value = TRUE)
} else {
# If searching in English columns
english_cols <- grep("^Industry_en", colnames(df), value = TRUE)
chinese_cols <- grep("^Industry_zh", colnames(df), value = TRUE)
}
# Perform regex matching in English columns
matches <- lapply(english_cols, function(col) {
grep(regex, df[[col]], value = TRUE, ignore.case=T)
})
# Extract corresponding Chinese values
chinese_values <- lapply(chinese_cols, function(col) {
df[[col]][unlist(matches)]
})
# Return a list of matched English values and corresponding Chinese values
result <- list(english = unlist(matches), chinese = unlist(chinese_values))
return(result)
}
match_regex <- function(regex, is_chinese = FALSE) {
dataframe <- select(hurun,starts_with("Industry_en"),starts_with("Industry_zh"))
if (is_chinese) {
# If searching in Chinese columns
english_cols <- grep("^Industry_en", colnames(dataframe), value = TRUE)
chinese_cols <- grep("^Industry_zh", colnames(dataframe), value = TRUE)
} else {
# If searching in English columns
english_cols <- grep("^Industry_zh", colnames(dataframe), value = TRUE)
chinese_cols <- grep("^Industry_en", colnames(dataframe), value = TRUE)
}
# Perform regex matching in English columns
matches <- lapply(english_cols, function(col) {
regex_matches <- grep(regex, dataframe[[col]], value = TRUE)
if (length(regex_matches) == 0) {
# If no matches found, return NA
return(NA)
} else {
# Extract corresponding Chinese values from Chinese columns
chinese_col <- chinese_cols[grep(col, colnames(dataframe), value = TRUE)]
return(dataframe[[chinese_col]][regex_matches])
}
})
# Return a list of matched English values and corresponding Chinese values
result <- list(english = unlist(matches), chinese = unlist(matches))
return(result)
}
match_regex("fiber optics cable")
matches <- lapply(english_cols, function(col) {
regex_matches <- grep(regex, dataframe[[col]], value = TRUE)
matches <- lapply(english_cols, function(col) {
regex_matches <- grep(regex, dataframe[[col]], value = TRUE)
matches
matches <- lapply(english_cols, function(col) {
regex_matches <- grep(regex, dataframe[[col]], value = TRUE)})
hurun <- readRDS("/Users/stephenschick/Library/Mobile Documents/com~apple~CloudDocs/School Work/Current School Work/POIR 641/Common Prosperity/Data for Common Prosperity/Hurun Rich List/hurun_rich_list_df.rds")
library(tidyverse)
library(quanteda)
library(topicmodels)
library(rvest)
library(stringr)
library(httr)
library(rlang)
industries <- hurun %>%
select(starts_with("Industry_en")) %>%
pivot_longer(cols = everything(),
names_to = NULL,
values_to = "business") %>%
drop_na() %>%
mutate(business = gsub("^ ","",business)) %>%
unique() %>%
mutate(subsector = case_when(grepl("search engine|mobile service|online games|software|electronic payment system|e-commerce",business,ignore.case = T)
~ "101010 Software and Computer Services",
grepl("electronic product|electronic appliances|micro motors|chips|computer|high tech|high-tech|optical cable",business,ignore.case = T)
~ "101020 Technology Hardware and Equipment",
grepl("telephone design|fiber optic cable|optical fibre cable|information equipment|ic card phone",business,ignore.case = T)
~ "151010 Telecommunications Equipment",
grepl("telecoms|sms-messaging|smsmessaging|communication|internet",business,ignore.case = T)
~ "151020 Telecommunications Service Providers",
grepl("health care",business,ignore.case = T)
~ "201010 Health Care Providers",
grepl("pharmaceutical|medicine|health product|biotechnology",business,ignore.case = T)
~ "201030 Pharmaceuticals, Biotechnology and Marijuana Producers",
grepl("financ*",business,ignore.case = T)
~ "302010 Finance and Credit Services",
grepl("^investment$|diversif|(invest)",business,ignore.case = T)
~ "302020 Investment Banking and Brokerage Services",
grepl("insurance",business,ignore.case = T)
~ "303020 Nonlife Insurance",
grepl("real estate|property",business,ignore.case =T)
~ "351010 Real Estate Investment and Services Development",
grepl("agency|motors|pickup trucks|rechargeable batter|(auto)|motorcycle|(bus)|axles|(car)|(tire)|(tires)",business,ignore.case =T)
~ "401010 Automobiles and Parts",
grepl("storage|(heating)|printing|education",business,ignore.case = T)
~ "402010 Consumer Services",
grepl("fridge|refrigerators|household electronic products|baby carriage|oven|microwave|wooden|woodwork|furniture|wood carving|air conditioner|electrical appliance|household appliance|home appliance",business,ignore.case = T)
~ "402020 Household Goods and Home Construction",
grepl("bowling|(tvs)|television|bowling equipment|bicycle",business,ignore.case = T)
~ "402030 Leisure Goods",
grepl("(watch)|footware|clothes|shoes|sandalwood|apparel|cashmere|jewelry|textile|clothing",business,ignore.case = T)
~ "402040 Personal Goods",
grepl("advertising|culture|entertainment|media",business,ignore.case = T)
~ "403010 Media",
grepl("shopping|retail|wholesale",business,ignore.case = T)
~ "404010 Retailers",
grepl("football clubs|soccer|sports|hotel|travel|taxi|tourism|golf course",business,ignore.case = T)
~ "405010 Travel and Leisure",
grepl("wine|drink|beverage|baijiu|liquor|liquour|juice",business,ignore.case = T)
~ "451010 Beverages",
grepl("noodles|pork|pudding|corn|agricultural|vegetable|aquaculture|slaughter|food|breeding|farm|seed|feed|meat|dairy|fish|livestock|beef|mutton|animal husbandry|agriculture",business,ignore.case = T)
~ "451020 Food Producers",
grepl("make-up|hygiene products|confectioneries|supermarket|cosmetics|shampoo|beauty care",business,ignore.case = T)
~ "452010 Personal Care, Drug and Grocery Stores",
grepl("piping|tile|pipes|drainage system|infrastructure|construction|concrete|cement|building material|flooring wood|(door)",business,ignore.case = T)
~ "501010 Construction and Materials",
grepl("aviation",business,ignore.case = T)
~ "502010 Aerospace and Defense",
grepl("industrial electric|^wiring$|^cable$|industrial electric equipment|electronic equipment|electronics|electrical",business,ignore.case = T)
~ "502020 Electronic and Electrical Equipment",
grepl("(dyes)|(dies)|(cans)|(pvc)|coating|glass|foam|plastic|fireproof material|packaging|paint|ceramics|new materials",business,ignore.case = T)
~ "502030 General Industrials",
grepl("fire control equipment|^industrial$|precision parts|spinning mill|tools|^equipment$|^machine$|^manufacturing$|machinery|pump|industrial appliances|machine tools|engineering|engine|boiler|valve|precision instruments|special equipment",business,ignore.case = T)
~ "502040 Industrial Engineering",
grepl("environmental protection|stamps|stationery|(it)|system integration",business,ignore.case = T)
~ "502050 Industrial Support Services",
grepl("port|logistics|cranes|earthmoving vehicles|trad*|transportation|distribution|road|supply|speedway",business,ignore.case = T)
~ "502060 Industrial Transportation",
grepl("wood product|wood industry|paperboard|(paper)|magnetic material|timber|silk",business,ignore.case = T)
~ "551010 Industrial Materials",
grepl("iron|primary metals|^processing$|rare earth|industrial diamonds|aluminum|aluminium|coal|steel",business,ignore.case = T)
~ "551020 Industrial Metals and Mining",
grepl("^gold$|diamond products",business,ignore.case = T)
~ "551030 Precious Metals and Mining",
grepl("special material|fertilizer|refrigerant|chemical|alumina",business,ignore.case = T)
~ "552010 Chemicals",
grepl("solar|recycled energy",business,ignore.case = T)
~ "601020 Renewable Energy",
grepl("electricity|power",business,ignore.case = T)
~ "651010 Electricity",
grepl("water|gas supply|natural gas",business,ignore.case = T)
~ "651020 Gas, Water and Multi-utilities",
TRUE ~ "MISSING")) %>%
filter(subsector == "MISSING")
View(industries)
debugSource("~/Library/Mobile Documents/com~apple~CloudDocs/School Work/Current School Work/POIR 641/2023-02-17 Common Prosperity.R")
debugSource("~/Library/Mobile Documents/com~apple~CloudDocs/School Work/Current School Work/POIR 641/2023-02-17 Common Prosperity.R")
setwd("/Users/stephenschick/Dropbox (Personal)/RA_Stephen/DDE_Propaganda Strategies/Twitter Data")
library("httr")
library("jsonlite")
library("dplyr")
library("academictwitteR")
library("lubridate")
install.packages("rtweet")
library("rtweet")
## news media baseline scraper ##
auth <- rtweet_app()
auth_save(auth, "BEARER")
help(tweets_data)
help(get_all_tweets)
help("resume_collection")
help("get_all_tweets")
help("build_query")
query <- build_query(users = user_list,
is_retweet = F,
is_reply = F,
is_quote = F,
is_verified = F,
lang = "en")
user_list <- c("CNN","BBCWorld","AJEnglish","nytimes","XHNews")
query <- build_query(users = user_list,
is_retweet = F,
is_reply = F,
is_quote = F,
is_verified = F,
lang = "en")
load("/Users/stephenschick/Dropbox (Personal)/RA_Stephen/DDE_Propaganda Strategies/Text Analysis/allusers_textanalysis.RDATA")
View(tweets)
max(tweets$POSIX)
tweets <-
get_all_tweets(
query = query,
start_tweets = "2023-01-01T00:00:00Z",
end_tweets = "2023-02-01T00:00:00Z",
file = "newstweets",
data_path = "/Users/stephenschick/Dropbox (Personal)/RA_Stephen/DDE_Propaganda Strategies/Twitter Data by User (baseline)",
n = 100,
)
Sys.setenv(TWITTER_BEARER = "AAAAAAAAAAAAAAAAAAAAAKIslgEAAAAAwIXXp1rZa2BXolt%2FCtsFcSlX%2B7Y%3DgVtOHi12cYFPZRIR5MAb59UHRBBHnT8csS0dA8tt1fwSXZDkjN")
tweets <-
get_all_tweets(
query = query,
start_tweets = "2023-01-01T00:00:00Z",
end_tweets = "2023-02-01T00:00:00Z",
file = "newstweets",
data_path = "/Users/stephenschick/Dropbox (Personal)/RA_Stephen/DDE_Propaganda Strategies/Twitter Data by User (baseline)",
n = 100,
bearer_token = get_bearer()
)
query <- build_query(users = user_list,
is_retweet = F,
is_reply = F,
is_quote = F,
lang = "en")
tweets <-
get_all_tweets(
query = query,
start_tweets = "2023-01-01T00:00:00Z",
end_tweets = "2023-02-01T00:00:00Z",
file = "newstweets",
data_path = "/Users/stephenschick/Dropbox (Personal)/RA_Stephen/DDE_Propaganda Strategies/Twitter Data by User (baseline)",
n = 100,
bearer_token = get_bearer()
)
View(tweets)
get_all_tweets(
query = query,
start_tweets = "2023-01-01T00:00:00Z",
end_tweets = "2023-02-01T00:00:00Z",
file = "newstweets",
data_path = "/Users/stephenschick/Dropbox (Personal)/RA_Stephen/DDE_Propaganda Strategies/Twitter Data by User (baseline)",
n = ,
bearer_token = get_bearer())
help("get_all_tweets")
user_list <- c("TheEconomist","RT_com","SkyNews","WSJ","washingtonpost","ndtv","AP","HuffPost","FinancialTimes","BBCAfrica",
"FRANCE24","SCMPNews","USAmbChina","NikkeiAsia","News24","guardian","Independent")
query <- build_query(users = user_list,
is_retweet = F,
is_reply = F,
is_quote = F,
lang = "en")
library("httr")
library("jsonlite")
library("dplyr")
library("academictwitteR")
library("lubridate")
library("rtweet")
query <- build_query(users = user_list,
is_retweet = F,
is_reply = F,
is_quote = F,
lang = "en")
quet
query
get_all_tweets(
query = query,
start_tweets = "2009-01-01T00:00:00Z",
end_tweets = "2023-02-01T00:00:00Z",
data_path = "/Users/stephenschick/Dropbox (Personal)/RA_Stephen/DDE_Propaganda Strategies/Twitter Data by User (baseline)/Extended_2023-2009",
n = 9000000,
bearer_token = get_bearer(),
bind_tweets = F)
Sys.setenv(TWITTER_BEARER = "AAAAAAAAAAAAAAAAAAAAAKIslgEAAAAAwIXXp1rZa2BXolt%2FCtsFcSlX%2B7Y%3DgVtOHi12cYFPZRIR5MAb59UHRBBHnT8csS0dA8tt1fwSXZDkjN")
get_all_tweets(
query = query,
start_tweets = "2009-01-01T00:00:00Z",
end_tweets = "2023-02-01T00:00:00Z",
data_path = "/Users/stephenschick/Dropbox (Personal)/RA_Stephen/DDE_Propaganda Strategies/Twitter Data by User (baseline)/Extended_2023-2009",
n = 9000000,
bearer_token = get_bearer(),
bind_tweets = F)
resume_collection(data_path = "/Users/stephenschick/Dropbox (Personal)/RA_Stephen/DDE_Propaganda Strategies/Twitter Data by User (baseline)/Extended_2023-2009",
bearer_token = get_bearer())
tweets <- bind_tweets(data_path = "/Users/stephenschick/Dropbox (Personal)/RA_Stephen/DDE_Propaganda Strategies/Twitter Data by User (baseline)/Extended_2023-2009")
min(tweets$created_at)
max(tweets$created_at)
get_all_tweets(
query = query,
start_tweets = "2009-01-01T00:00:00Z",
end_tweets = "2020-09-11T14:27:43.000Z",
data_path = "/Users/stephenschick/Dropbox (Personal)/RA_Stephen/DDE_Propaganda Strategies/Twitter Data by User (baseline)/Extended_2023-2009",
n = 9000000,
bearer_token = get_bearer(),
bind_tweets = F)
library("httr")
library("jsonlite")
library("dplyr")
library("academictwitteR")
library("lubridate")
tweets <- bind_tweets(data_path = "/Users/stephenschick/Dropbox (Personal)/RA_Stephen/DDE_Propaganda Strategies/Twitter Data by User (baseline)/Binding")
tweets <- bind_tweets(data_path = "/Users/stephenschick/Dropbox (Personal)/RA_Stephen/DDE_Propaganda Strategies/Twitter Data by User (baseline)/Binding")
install.packages("parallel")
library("parallel")
detectCores()
detectCores(logical = FALSE)
gc()
names(tweets)
saveRDS(tweets,file = "/Users/stephenschick/Dropbox (Personal)/RA_Stephen/DDE_Propaganda Strategies/Twitter Data by User (baseline)/twitter_baseline_full.rds")
tweets_sub <- tweets %>%
select(text,id,author_id,created_at)
saveRDS(tweets_sub,file = "/Users/stephenschick/Dropbox (Personal)/RA_Stephen/DDE_Propaganda Strategies/Twitter Data by User (baseline)/twitter_baseline_text.rds")
x <- "https://www.myneta.info/andhrapradesh2019/index.php?action=summary&subAction=candidates_analyzed&sort=candidate&page=1"
url <- z
url <- x
html <- read_html(url)
library(tidyverse)
library(rvest, warn.conflicts=FALSE)
library(stringr)
table <- html_nodes("body > div.w3-container > div.w3-responsive > table") %>%
html_table()
html <- read_html(url)
table <- html_nodes("body > div.w3-container > div.w3-responsive > table") %>%
html_table()
table <- html %>%
html_nodes("body > div.w3-container > div.w3-responsive > table") %>%
html_table()
View(table)
View(table)
table[1]
df = table[1]
df$cand_url = html_nodes(candidates, ".w3-bordered a") %>%
html_text()
df$cand_url = html %>%
html_nodes(".w3-bordered a") %>%
html_text()
View(df)
df = table[1]
df = tibble(table[1])
df = tibble(table[[1]])
df$cand_url = html %>%
html_nodes(".w3-bordered a") %>%
html_text()
View(df)
df$cand_url = html %>%
html_nodes(".w3-bordered a")  %>%
html_attr("href") %>% paste("https://www.myneta.info/andhrapradesh2019/", ., sep="")
View(df)
scrape_cand = function(url) {
html = read_html(url)
table = html %>%
html_nodes("body > div.w3-container > div.w3-responsive > table") %>%
html_table()
df = tibble(table[[1]])
df$cand_url = html %>%
html_nodes(".w3-bordered a")  %>%
html_attr("href") %>% paste("https://www.myneta.info/andhrapradesh2019/", ., sep="")
}
candidate_df = lapply(1,function(i){
url <- paste0("https://www.myneta.info/andhrapradesh2019/index.php?action=summary&subAction=candidates_analyzed&sort=candidate&page=",i)
df <- scrape_cand(url)
return(df)
})
View(candidate_df)
scrape_cand = function(url) {
html = read_html(url)
table = html %>%
html_nodes("body > div.w3-container > div.w3-responsive > table") %>%
html_table()
df = tibble(table[[1]])
df$cand_url = html %>%
html_nodes(".w3-bordered a")  %>%
html_attr("href") %>% paste("https://www.myneta.info/andhrapradesh2019/", ., sep="")
return(df)
}
candidate_df = lapply(1,function(i){
url <- paste0("https://www.myneta.info/andhrapradesh2019/index.php?action=summary&subAction=candidates_analyzed&sort=candidate&page=",i)
df <- scrape_cand(url)
return(df)
})
View(candidate_df)
View(candidate_df)
candidate_df_raw = lapply(1:11,function(i){
url = paste0("https://www.myneta.info/andhrapradesh2019/index.php?action=summary&subAction=candidates_analyzed&sort=candidate&page=",i)
df = scrape_cand(url)
return(df)
})
candidate_df = candidate_df_raw %>%
bind_rows()
View(candidate_df)
candidate_df_raw = lapply(1:11,function(i){
url = paste0("https://www.myneta.info/andhrapradesh2019/index.php?action=summary&subAction=candidates_analyzed&sort=candidate&page=",i)
df = scrape_cand(url)
return(df)
}) %>% bind_rows()
View(candidate_df)
# data cleaning
candidate_df = candidate_df_raw %>%
separate(col = `Total Assets`,into = c("assets","assets_other"),sep = "~") %>%
mutate(assets = str_remove_all(assets,"[Rs ]"),
assets = str_remove_all(assets,"[,]")) %>%
mutate(assets = if_else(assets == "Nil","0",assets)) %>%
mutate(assets = as.numeric(str_trim(assets)))
View(candidate_df)
candidate_df = candidate_df %>%
separate(col = `Liabilities`,into = c("liabilities","liabilities_other"),sep = "~") %>%
mutate(liabilities = str_remove_all(liabilities, "[Rs]"),
liabilities = str_remove_all(liabilities,"[,]")) %>%
mutate(liabilities = if_else(liabilities == "Nil","0",liabilities)) %>%
mutate(liabilities = as.numeric(str_trim(liabilities)))
candidate_df = candidate_df %>%
mutate(age = str_remove_all(age, "Age: "),
age = str_trim(age))
## add age
page <- c()
for (i in 1:length(candidate_df_raw)) {
page[i] = read_html(candidate_df_raw$cand_url[i])
candidate_df_raw$age = html_nodes(page[i],".w3-panel div:nth-child(5)") %>% html_text()
}
i <- 1
page[i] = read_html(candidate_df_raw$cand_url[i])
candidate_df_raw$cand_url[i]
page = read_html(candidate_df_raw$cand_url)
## add age
page <- list()
## add age
page = c()
page[i] = read_html(candidate_df_raw$cand_url[i])
page[[i]] = read_html(candidate_df_raw$cand_url[i])
page[[i]] = read_html(candidate_df_raw$cand_url[i])
candidate_df_raw$age = html_nodes(page[i],".w3-panel div:nth-child(5)") %>% html_text()
candidate_df_raw$age = html_nodes(page[[i]],".w3-panel div:nth-child(5)") %>% html_text()
View(candidate_df)
View(candidate_df_raw)
## add age
html = lapply(candidate_df_raw$cand_url,read_html)
library(future.apply)
## add age (takes much longer because we're pulling html from individuals' pages)
html = future.lapply(candidate_df_raw$cand_url[1],read_html)
library(future.apply)
## add age (takes much longer because we're pulling html from individuals' pages)
html = future.lapply(candidate_df_raw$cand_url[1],read_html)
## add age (takes much longer because we're pulling html from individuals' pages)
html = future_lapply(candidate_df_raw$cand_url[1],read_html)
View(html)
## add age (takes much longer because we're pulling html from individuals' pages)
html = future_lapply(candidate_df_raw$cand_url,read_html)
## add age (takes much longer because we're pulling html from individuals' pages)
age = future_vapply(candidate_df_raw$cand_url,function(url){
age = read_html(url) %>%
html_nodes(html,".w3-panel div:nth-child(5)") %>%
html_text()
return(age))
## add age (takes much longer because we're pulling html from individuals' pages)
age = future_vapply(candidate_df_raw$cand_url,function(url){
age = read_html(url) %>%
html_nodes(html,".w3-panel div:nth-child(5)") %>%
html_text()
return(age)})
## add age (takes much longer because we're pulling html from individuals' pages)
age = future_vapply(candidate_df_raw$cand_url,FUN = function(url){
age = read_html(url) %>%
html_nodes(".w3-panel div:nth-child(5)") %>%
html_text()
return(age)
})
candidate_df_raw$cand_url
## add age (takes much longer because we're pulling html from individuals' pages)
age = future_vapply(candidate_df_raw$cand_url,FUN = function(url){
age = read_html(url) %>%
html_nodes(".w3-panel div:nth-child(5)") %>%
html_text()
return(age)
})
## add age (takes much longer because we're pulling html from individuals' pages)
age = future_lapply(candidate_df_raw$cand_url,FUN = function(url){
age = read_html(url) %>%
html_nodes(".w3-panel div:nth-child(5)") %>%
html_text()
return(age)
})
v <- c(3.3,3.7,4,2,4,4,4,4,4,4,3.7,4,4,4)
sum(v) / 13.5
library(tidyverse)
library(readxl)
firm <- readxl("/Users/stephenschick/Library/Mobile Documents/com~apple~CloudDocs/School Work/Current School Work/Substantive Paper/Chinese Firm Data/CnOpenData工商企业信息数据（样本）/basic-1.xlsx")
firm <- read_excel("/Users/stephenschick/Library/Mobile Documents/com~apple~CloudDocs/School Work/Current School Work/Substantive Paper/Chinese Firm Data/CnOpenData工商企业信息数据（样本）/basic-1.xlsx")
View(firm)
inspection <- read_excel("/Users/stephenschick/Library/Mobile Documents/com~apple~CloudDocs/School Work/Current School Work/Substantive Paper/Chinese Firm Data/CnOpenData工商企业信息数据（样本）/randominspection-1.xlsx")
View(inspection)
firm_ids <- unique(firm$companyId)
firm_ids <- unique(inspection$companyId)
summary(as.factor(inspection$inspectionType))
class(inspection$inspectionDate)
inspection$inspectionDate <- as.Date(inspection$inspectionDate)
inspection$inspectionDate <- as.POSIXct(inspection$inspectionDate)
test <- inspection %>%
mutate(Date = as.Date(inspectionDate,origin = "1899-12-30"))
View(test)
test <- inspection %>%
mutate(Date = if_else(inspectionDate < 10000, paste0("01/01/",inspectionDate),as.Date(inspectionDate,origin = "1899-12-30")))
test <- inspection %>%
mutate(Date = if_else(inspectionDate < 10000,as.Date.character(paste0("01/01/",inspectionDate)),as.Date(inspectionDate,origin = "1899-12-30")))
?as.Date.character
test <- inspection %>%
mutate(Date = if_else(inspectionDate < 10000,as.Date.character(paste0("01/01/",inspectionDate),format = "MM/DD/YYYY"),as.Date(inspectionDate,origin = "1899-12-30")))
View(test)
test <- inspection %>%
mutate(Date = if_else(inspectionDate < 10000,as.Date.character(paste0("01/01/",inspectionDate),format = "YYYY-MM-DD"),as.Date(inspectionDate,origin = "1899-12-30")))
?cor
?corr
?cov
library("haven")
df <- read_dta("/Users/stephenschick/Downloads/Koenig_Archibugi_Replication(1)/data/RTB_AGMKA.dta")
View(df)
df <- load("/Users/stephenschick/Library/CloudStorage/GoogleDrive-saschick@usc.edu/My Drive/POIR/Dyadic IPE Data Resource/Bilateral_IPE_v1.RDATA")
View(ipe_bilat)
?summarise
load("/Users/stephenschick/Library/CloudStorage/GoogleDrive-saschick@usc.edu/.shortcut-targets-by-id/1sT18DJ_Z2M1DcZCSku436FI2qd2TcgnS/AppendID (Copy of the Original)/Append_ids/MasterGWNO.RDATA")
View(ids)
install.packages(c("devtools", "roxygen2","usethis", "testthat"))
options(repos = list(CRAN="http://cran.rstudio.com/"))
pkgs <- available.packages(filters = c("CRAN", "duplicates"))[,'Package']
library("devtools")
options(repos = list(CRAN="http://cran.rstudio.com/"))
pkgs <- available.packages(filters = c("CRAN", "duplicates"))[,'Package']
?available.packages
pkgs <- available.packages(filters = c("CRAN", "duplicates"))[,'Package']
load("/Users/stephenschick/Library/CloudStorage/GoogleDrive-saschick@usc.edu/.shortcut-targets-by-id/1sT18DJ_Z2M1DcZCSku436FI2qd2TcgnS/AppendID (Copy of the Original)/Append_ids/MasterGWNO.RDATA")
library("devtools")
use_r("AppendIDs_FunctionsSum_SS_20240224")
setwd("/Users/stephenschick/Documents/GitHub/appendIDs/R/")
use_r("AppendIDs_FunctionsSum_SS_20240224")
use_data(ids, internal = TRUE)
#Load in the country IDs file
load("sysdata.rda")
